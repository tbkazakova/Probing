{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ROSehDDkfka"
   },
   "source": [
    "# Скачиваем полезное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OmoUGv6Qkfk7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: razdel in /usr/local/lib/python3.9/site-packages (0.5.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I3mvVlRVkflE",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pymorphy2 in /usr/local/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.9/site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.9/site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.9/site-packages (from pymorphy2) (0.7.2)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kNxwNfMAkflE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/site-packages (0.14.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from torchvision) (1.23.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (2022.6.15)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-qkgbzxJkflG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/site-packages (4.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2022.6.15)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UdaZ2zeakflL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: evaluate in /usr/local/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/site-packages (from evaluate) (0.11.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from evaluate) (1.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from evaluate) (1.23.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/site-packages (from evaluate) (2.8.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/site-packages (from evaluate) (2022.11.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas->evaluate) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uo6-7PcLkflP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/site-packages (4.25.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.9/site-packages (1.7.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (5.9.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from datasets) (1.5.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets faiss-cpu psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od5nkvxskflY"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/AIRI-Institute/Probing_framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1672278642123,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "DkhqZ9AHkflc"
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8055,
     "status": "ok",
     "timestamp": 1672278650174,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "aTtBU4Y-kflg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import math\n",
    "from functools import reduce\n",
    "from time import time\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "from razdel import tokenize\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import unittest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from mlm_pytorch.mlm_pytorch import MLM\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers.utils import logging\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1672278650176,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "94PxZ5sHkflk"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1672278650177,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "3dZ0uIQMkflk"
   },
   "outputs": [],
   "source": [
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1672278687401,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "e1F8jg1ikflk"
   },
   "outputs": [],
   "source": [
    "# from probing.classifier import MLP, LogReg, MDLLinearModel\n",
    "# from probing.data_former import TextFormer\n",
    "# from probing.encoder import TransformersLoader\n",
    "# from probing.metric import Metric\n",
    "# from probing.utils import KL_Loss, ProbingLog, lang_category_extraction, save_log\n",
    "\n",
    "# logging.set_verbosity_warning()\n",
    "# logger = logging.get_logger(\"probing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpoWMkxHkflm"
   },
   "source": [
    "# Портим текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1672278746333,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "ukZ2R1tfkfln"
   },
   "outputs": [],
   "source": [
    "class TextIlliteracy:\n",
    "    \"\"\"Class for tokenizing text and changing variant of grammatical category\n",
    "       in words of chosen parts of speech\"\"\"\n",
    "    _tokens: List[str]\n",
    "\n",
    "    def __init__(self, text: str) -> None:\n",
    "        \"\"\"Initializes an object\"\"\"\n",
    "        self._text = text\n",
    "        self._tokens = []\n",
    "    \n",
    "    def get_original_text(self) -> str:\n",
    "        return self._text\n",
    "\n",
    "    def tokenize_text(self) -> List[str]:\n",
    "        \"\"\"Tokenizes text\"\"\"\n",
    "        pass\n",
    "\n",
    "    def spoil_text(self, gram: str, postag_list: List[str]) -> str:\n",
    "        \"\"\"Changes grammatical markers to random inside the choosen category\n",
    "           in all words of choosen parts of speech,\n",
    "           if this grammatical category is relevant for such POS\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1672278754586,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "4_Tc_ZMCkflo"
   },
   "outputs": [],
   "source": [
    "class TextIlliteracyRus(TextIlliteracy):\n",
    "    \"\"\"Class tokenize russian text and changes variant of grammatical category\n",
    "       in words of chosen parts of speech.\"\"\"\n",
    "    \n",
    "    _tokens: List[str]\n",
    "    # https://pymorphy2.readthedocs.io/en/stable/user/grammemes.html#grammeme-docs\n",
    "    \n",
    "    __postags = [\"NOUN\", \"ADJF\", \"ADJS\", \"COMP\", \"VERB\", \"INFN\", \"PRTF\", \"PRTS\",\n",
    "                 \"GRND\", \"NUMR\", \"ADVB\", \"NPRO\", \"PRED\", \"PREP\", \"CONJ\", \"PRCL\", \"INTJ\"]\n",
    "    \n",
    "    __grams = {\"nmbr\": [\"sing\", \"plur\"],\n",
    "               \"case\": [\"nomn\", \"gent\", \"datv\", \"accs\", \"ablt\",\n",
    "                        \"loct\", \"voct\", \"gen2\", \"acc2\", \"loc2\"],\n",
    "               \"anim\": [\"anim\", \"inan\"],\n",
    "               \"gndr\": [\"masc\", \"femn\", \"neut\", \"ms-f\"],\n",
    "               \"aspc\": [\"perf\", \"impf\"],\n",
    "               \"trns\": [\"tran\", \"intr\"],\n",
    "               \"pers\": [\"1per\", \"2per\", \"3per\"],\n",
    "               \"tens\": [\"pres\", \"past\", \"futr\"],\n",
    "               \"mood\": [\"indc\", \"impr\"],\n",
    "               \"invl\": [\"incl\", \"excl\"],\n",
    "               \"voic\": [\"actv\", \"pssv\"]}\n",
    "\n",
    "    def tokenize_text(self) -> List[str]:\n",
    "        \"\"\"For russian:\n",
    "           tokenizes text\"\"\"\n",
    "        if self._tokens != []:\n",
    "            tokens = self._tokens\n",
    "        else:\n",
    "            tokens_with_boundaries = list(tokenize(self._text))\n",
    "            # получили список токенов с границами\n",
    "            tokens = [] # список токенов с пробелами в нужных местах\n",
    "            prev_tok_end = 0\n",
    "            for substring in tokens_with_boundaries:\n",
    "                if substring.start != prev_tok_end:\n",
    "                    tokens.append(\" \")\n",
    "                tokens.append(substring.text)\n",
    "                prev_tok_end = substring.stop\n",
    "            self._tokens = tokens\n",
    "        return tokens\n",
    "        \n",
    "    def spoil_text(self, gram: str=\"nmbr\", postag_list: List[str]=__postags) -> str:\n",
    "        \"\"\"For russian:\n",
    "           changes grammatical markers to random inside the choosen category\n",
    "           in all words of choosen parts of speech,\n",
    "           if this grammatical category is relevant for such POS\"\"\"\n",
    "        # берёт список частей речи и категорию,\n",
    "        # которую у этих частей речи надо портить рандомными вариантами\n",
    "        \n",
    "        if self._tokens == []:\n",
    "            self._tokens = self.tokenize_text()\n",
    "\n",
    "        tokens = self._tokens\n",
    "\n",
    "        changed_list = []\n",
    "        for tok in tokens:\n",
    "            tok_analysed = morph.parse(tok)[0]\n",
    "            if tok_analysed.tag.POS in postag_list:\n",
    "                new_gram_val = random.choice(TextIlliteracyRus.__grams[gram])\n",
    "                if tok_analysed.inflect({new_gram_val}) is not None:\n",
    "                    changed_tok = tok_analysed.inflect({new_gram_val}).word\n",
    "                    if tok[0].isupper():\n",
    "                        # чтобы при изменении буква оставалась заглавной в т.ч. для слова \"Ма́трица\"\n",
    "                        changed_tok = changed_tok[0].upper()+changed_tok[1:]\n",
    "                    changed_list.append(changed_tok)\n",
    "                else:\n",
    "                    changed_list.append(tok)\n",
    "            else:\n",
    "                changed_list.append(tok)\n",
    "        changed_text = \"\".join(changed_list)\n",
    "        return changed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1672278809990,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "XHRudroskfls"
   },
   "outputs": [],
   "source": [
    "# для других языков порча текста может основываться на других специализированных библиотеках\n",
    "# в том числе на трансдьюсерах\n",
    "# class TextIlliteracyEng, например"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sdnv5Z9kfls"
   },
   "source": [
    "## Тестирование\n",
    "\n",
    "(смотреть удобнее тут, но .py файл тоже есть.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1672278812565,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "4vRFmgzOkfls"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Ма́трица — математический объект, записываемый в виде прямоугольной таблицы элементов кольца или поля (например, целых, действительных или комплексных чисел), который представляет собой совокупность строк и столбцов, на пересечении которых находятся его элементы. Количество строк и столбцов задает размер матрицы. Хотя исторически рассматривались, например, треугольные матрицы[1], в настоящее время говорят исключительно о матрицах прямоугольной формы, так как они являются наиболее удобными и общими.\n",
    "Матрицы широко применяются в математике для компактной записи систем линейных алгебраических или дифференциальных уравнений. В этом случае количество строк матрицы соответствует числу уравнений, а количество столбцов — количеству неизвестных. В результате решение систем линейных уравнений сводится к операциям над матрицами.\"\"\"\n",
    "tokens = ['Ма́трица', ' ', '—', ' ', 'математический', ' ', 'объект', ',', ' ', 'записываемый', ' ', 'в', ' ', 'виде',\n",
    "          ' ', 'прямоугольной', ' ', 'таблицы', ' ', 'элементов', ' ', 'кольца', ' ', 'или', ' ', 'поля', ' ', '(',\n",
    "          'например', ',', ' ', 'целых', ',', ' ', 'действительных', ' ', 'или', ' ', 'комплексных', ' ', 'чисел', ')',\n",
    "          ',', ' ', 'который', ' ', 'представляет', ' ', 'собой', ' ', 'совокупность', ' ', 'строк', ' ', 'и', ' ',\n",
    "          'столбцов', ',', ' ', 'на', ' ', 'пересечении', ' ', 'которых', ' ', 'находятся', ' ', 'его', ' ', 'элементы',\n",
    "          '.', ' ', 'Количество', ' ', 'строк', ' ', 'и', ' ', 'столбцов', ' ', 'задает', ' ', 'размер', ' ', 'матрицы',\n",
    "          '.', ' ', 'Хотя', ' ', 'исторически', ' ', 'рассматривались', ',', ' ', 'например', ',', ' ', 'треугольные', ' ',\n",
    "          'матрицы', '[', '1', ']', ',', ' ', 'в', ' ', 'настоящее', ' ', 'время', ' ', 'говорят', ' ', 'исключительно',\n",
    "          ' ', 'о', ' ', 'матрицах', ' ', 'прямоугольной', ' ', 'формы', ',', ' ', 'так', ' ', 'как', ' ', 'они', ' ',\n",
    "          'являются', ' ', 'наиболее', ' ', 'удобными', ' ', 'и', ' ', 'общими', '.', ' ', 'Матрицы', ' ', 'широко',\n",
    "          ' ', 'применяются', ' ', 'в', ' ', 'математике', ' ', 'для', ' ', 'компактной', ' ', 'записи', ' ', 'систем',\n",
    "          ' ', 'линейных', ' ', 'алгебраических', ' ', 'или', ' ', 'дифференциальных', ' ', 'уравнений', '.', ' ', 'В',\n",
    "          ' ', 'этом', ' ', 'случае', ' ', 'количество', ' ', 'строк', ' ', 'матрицы', ' ', 'соответствует', ' ', 'числу',\n",
    "          ' ', 'уравнений', ',', ' ', 'а', ' ', 'количество', ' ', 'столбцов', ' ', '—', ' ', 'количеству', ' ',\n",
    "          'неизвестных', '.', ' ', 'В', ' ', 'результате', ' ', 'решение', ' ', 'систем', ' ', 'линейных', ' ',\n",
    "          'уравнений', ' ', 'сводится', ' ', 'к', ' ', 'операциям', ' ', 'над', ' ', 'матрицами', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1672278815762,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "AV0hGjiukflu"
   },
   "outputs": [],
   "source": [
    "class TestTextIlliteracyRus(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        print(\"setting up an object\")\n",
    "        self.textilliteracyrus = TextIlliteracyRus(text)\n",
    "        \n",
    "    def test_get_original_text(self):\n",
    "        print(\"testing get_original_text\")\n",
    "        self.assertEqual(self.textilliteracyrus.get_original_text(), text)\n",
    "\n",
    "    def test_tokenize_text(self):\n",
    "        print(\"testing tokenize_text\")\n",
    "        self.assertEqual(self.textilliteracyrus.tokenize_text(), tokens)\n",
    "    \n",
    "    def test_spoil_text(self):\n",
    "        print(\"testing spoil_text\")\n",
    "        self.assertIsInstance(self.textilliteracyrus.spoil_text(), str)\n",
    "        self.assertIsInstance(self.textilliteracyrus.spoil_text(\"tens\"), str)\n",
    "        self.assertIsInstance(self.textilliteracyrus.spoil_text(\"case\", [\"NOUN\", \"ADJF\"]), str)\n",
    "        self.assertIsInstance(self.textilliteracyrus.spoil_text(\"tens\", [\"NOUN\", \"ADJF\"]), str)\n",
    "        \n",
    "    def test_hidden(self):\n",
    "        # print(\"testing hidden _text\")\n",
    "        # with self.assertRaises(AttributeError):\n",
    "        #     self.textilliteracyrus._text\n",
    "        # print(\"testing hidden _tokens\")\n",
    "        # with self.assertRaises(AttributeError):\n",
    "        #     self.textilliteracyrus._tokens\n",
    "        print(\"testing hidden __postags\")\n",
    "        with self.assertRaises(AttributeError):\n",
    "            TextIlliteracyRus.__postags\n",
    "        print(\"testing hidden __grams\")\n",
    "        with self.assertRaises(AttributeError):\n",
    "            TextIlliteracyRus.__grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1672278819600,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "QADVvW63kfl6",
    "outputId": "1a1ed230-8d80-46f6-a92a-54f6af9deb44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_get_illiterate_UDtexts (__main__.TestIlliterateUD) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up an object\n",
      "testing get_illiterate_UDtexts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_get_original_text (__main__.TestTextIlliteracyRus) ... ok\n",
      "test_hidden (__main__.TestTextIlliteracyRus) ... ok\n",
      "test_spoil_text (__main__.TestTextIlliteracyRus) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up an object\n",
      "testing get_original_text\n",
      "setting up an object\n",
      "testing hidden __postags\n",
      "testing hidden __grams\n",
      "setting up an object\n",
      "testing spoil_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_tokenize_text (__main__.TestTextIlliteracyRus) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up an object\n",
      "testing tokenize_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 5.495s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x126fe95e0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KGNujGHkfl7"
   },
   "source": [
    "## Примеры работы класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1672278834014,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "tWwJlfYfkfl7",
    "outputId": "21fc800f-ae5e-414d-df87-78d092165b6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ма́трица — математический объект, записываемый в виде прямоугольной таблицы элементов кольца или поля (например, целых, действительных или комплексных чисел), который представляет собой совокупность строк и столбцов, на пересечении которых находятся его элементы. Количество строк и столбцов задает размер матрицы. Хотя исторически рассматривались, например, треугольные матрицы[1], в настоящее время говорят исключительно о матрицах прямоугольной формы, так как они являются наиболее удобными и общими.\\nМатрицы широко применяются в математике для компактной записи систем линейных алгебраических или дифференциальных уравнений. В этом случае количество строк матрицы соответствует числу уравнений, а количество столбцов — количеству неизвестных. В результате решение систем линейных уравнений сводится к операциям над матрицами.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = TextIlliteracyRus(text)\n",
    "a.get_original_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "error",
     "timestamp": 1672278837501,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "NCr0Yw2ukfl8",
    "outputId": "307f5e96-774c-47bf-937d-48413eea381d"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TextIlliteracyRus' has no attribute '__postags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# при неосторожности __postags и __grams не поменяются\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mTextIlliteracyRus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__postags\u001b[49m)\n\u001b[1;32m      3\u001b[0m TextIlliteracyRus\u001b[38;5;241m.\u001b[39m__postags \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNOUN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADJF\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADJS\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(TextIlliteracyRus\u001b[38;5;241m.\u001b[39m__postags)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'TextIlliteracyRus' has no attribute '__postags'"
     ]
    }
   ],
   "source": [
    "# при неосторожности __postags и __grams не поменяются\n",
    "print(TextIlliteracyRus.__postags)\n",
    "TextIlliteracyRus.__postags = ['NOUN', 'ADJF','ADJS']\n",
    "print(TextIlliteracyRus.__postags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXWfJzB6kfl8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.tokenize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1672278851429,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "wNpPXRzbkfl8",
    "outputId": "e5a5fd99-eb9d-4c42-97cd-dd645c606920"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ма́трице — математическим объекта, записываемый в вида прямоугольной таблицу элементов кольца или полем (например, целых, действительные или комплексных числам), которым представляет собой совокупности строки и столбцы, на пересечения которых находятся его элементы. Количества строки и столбцами задает размер матрице. Хотя исторически рассматривались, например, треугольных матрицы[1], в настоящего временем говорят исключительно о матрицах прямоугольной формы, так как они являются наиболее удобные и общим. Матрице широко применяются в математика для компактной записи систем линейными алгебраических или дифференциальным уравнения. В этом случая количество строками матрице соответствует числом уравнениям, а количестве столбцам — количество неизвестные. В результатом решение системам линейные уравнениях сводится к операции над матрицы.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.spoil_text(\"case\", [\"NOUN\", \"ADJF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1672278854784,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "eWmqzgdAkfl9",
    "outputId": "a257e121-482f-4982-885d-7f3516f73913"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ма́трица — математический объект, записываемый в виде прямоугольной таблицы элементов кольца или поля (например, целых, действительных или комплексных чисел), который представляет собой совокупность строк и столбцов, на пересечении которых находятся его элементы. Количество строк и столбцов задает размер матрицы. Хотя исторически рассматривались, например, треугольные матрицы[1], в настоящее время говорят исключительно о матрицах прямоугольной формы, так как они являются наиболее удобными и общими. Матрицы широко применяются в математике для компактной записи систем линейных алгебраических или дифференциальных уравнений. В этом случае количество строк матрицы соответствует числу уравнений, а количество столбцов — количеству неизвестных. В результате решение систем линейных уравнений сводится к операциям над матрицами.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Если у POS нет такой грам.категории текст просто не меняется.\n",
    "# Можно было бы ограничить, чтоб на нерелевантной паре прогамма бы ломалась,\n",
    "# но мой вариант мне больше нравится (это спорно, можно думать)\n",
    "a.spoil_text(\"tens\", [\"NOUN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1672278878970,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "leD8mxNzkfl-",
    "outputId": "aa2bd7a0-b2dd-45b7-e523-e398770ac3e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ма́трица — математический объект, записывавший в виде прямоугольной таблицы элементов кольца или поля (например, целых, действительных или комплексных чисел), который представляет собой совокупность строк и столбцов, на пересечении которых находятся его элементы. Количество строк и столбцов задает размер матрицы. Хотя исторически рассматривались, например, треугольные матрицы[1], в настоящее время говорят исключительно о матрицах прямоугольной формы, так как они являются наиболее удобными и общими. Матрицы широко применяются в математике для компактной записи систем линейных алгебраических или дифференциальных уравнений. В этом случае количество строк матрицы соответствует числу уравнений, а количество столбцов — количеству неизвестных. В результате решение систем линейных уравнений сводился к операциям над матрицами.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.spoil_text(\"tens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFkL_c_Xkfl_"
   },
   "source": [
    "# Получаем неправильные предложения для выбранных языков, для выбранных грамматических категорий в папки файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1672287591429,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "nNro7By5n5ZD"
   },
   "outputs": [],
   "source": [
    "class IlliterateUD:\n",
    "    __lang_textilliteracy = {\"rus\": TextIlliteracyRus}\n",
    "    def get_illiterate_UDtexts(self, gramcats: List[str]=[\"nmbr\"],\n",
    "                               lang_list: List[str]=[\"rus\"]) -> None:\n",
    "        \"\"\"Creates a folder with good sentences from UD for each language\"\"\"\n",
    "        for lang in lang_list:\n",
    "            UD_filepath = \"UD_sentences/\"+lang+\"_UD_sentences.csv\"\n",
    "            dirname = lang+\"_for_perplexity\"\n",
    "            if not os.path.exists(dirname): os.makedirs(dirname)\n",
    "            TextIlliteracyLang = IlliterateUD.__lang_textilliteracy[lang]\n",
    "            gramcat_spoiled_sent = {}\n",
    "            for gramcat in gramcats:\n",
    "                gramcat_spoiled_sent[gramcat] = []\n",
    "\n",
    "            with open(UD_filepath, 'r') as f:\n",
    "                sents = f.readlines()\n",
    "                notspoiledUD_filepath = dirname+\"/\"+lang+\".csv\"\n",
    "                with open(notspoiledUD_filepath, 'w') as f1:\n",
    "                    f1.write(\"\".join(sents[:100]))\n",
    "\n",
    "                for sent in sents[:100]:\n",
    "                    sent_obj = TextIlliteracyLang(sent.strip())\n",
    "                    for gramcat in gramcats:\n",
    "                        spoiled_sent = sent_obj.spoil_text(gramcat)\n",
    "                        gramcat_spoiled_sent[gramcat].append(spoiled_sent)\n",
    "            for gramcat in gramcats:\n",
    "                spoiledUD_filepath = dirname+\"/\"+lang+\"_spoiledUD_\"+gramcat+\".csv\"\n",
    "                with open(spoiledUD_filepath, 'w') as f2:\n",
    "                    f2.write(\"\\n\".join(gramcat_spoiled_sent[gramcat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1672287596317,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "ymf6_XtJpvX5"
   },
   "outputs": [],
   "source": [
    "b = IlliterateUD()\n",
    "b.get_illiterate_UDtexts([\"nmbr\", \"case\", \"pers\", \"tens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestIlliterateUD(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        print(\"setting up an object\")\n",
    "        self.illiterateUD = IlliterateUD()\n",
    "        \n",
    "    def test_get_illiterate_UDtexts(self, gramcats: List[str]=[\"nmbr\"],\n",
    "                                          lang_list: List[str]=[\"rus\"]) -> None:\n",
    "        print(\"testing get_illiterate_UDtexts\")\n",
    "        self.assertEqual(self.illiterateUD.get_illiterate_UDtexts(), None)\n",
    "        self.assertEqual(self.illiterateUD.get_illiterate_UDtexts([\"tens\", \"pers\", \"case\"]), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buyBw5Wu90kt"
   },
   "source": [
    "# Будущие классы для полной структуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1672284690309,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "ga5A1wrqkfmE"
   },
   "outputs": [],
   "source": [
    "class AIRI_Probing:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsMenOdokfmC"
   },
   "source": [
    "Здесь можно посмотреть Probing_framework:\n",
    "\n",
    "https://github.com/AIRI-Institute/Probing_framework/blob/46f7ebad3af63b2b3b07fbe99398ce3eb27ed2b7/probing/pipeline.py#L24\n",
    "\n",
    "https://github.com/AIRI-Institute/Probing_framework/blob/main/scripts/probing_experiment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1672284646450,
     "user": {
      "displayName": "Татьяна Казакова",
      "userId": "07828874048466328312"
     },
     "user_tz": -180
    },
    "id": "Cyqik3sJ7FmM"
   },
   "outputs": [],
   "source": [
    "class ConlluUDParser:\n",
    "    \"\"\"Creates folders with sentences from UD for each language\n",
    "       for chosen type of probing\"\"\"\n",
    "       # Надо переписать ConlluUDParser в фреймворке,\n",
    "       # чтобы можно было добывать данные для их задачи и моей задачи\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def get_good_UDtexts(self, lang_list: List[str]=[\"rus\"]) -> None:\n",
    "        \"\"\"Creates a folder with good sentences from UD for each language\n",
    "           for perplexity probing\"\"\"\n",
    "        # Здесь для каждого языка сохранются все предложения в папку UD_sentences\n",
    "        # Так русского в этой папке лежит rus_UD_sentences.csv\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ConlluUDParser()\n",
    "d.get_good_UDtexts([\"rus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_k52Ae1kfmC"
   },
   "source": [
    "# Перплексия\n",
    "(чем меньше, тем лучше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexityProbing(AIRI_Probing):\n",
    "    #lang_model_name = {\"rus\": \"distilbert-base-multilingual-cased\"}\n",
    "\n",
    "    def __init__(self, gramcats: List[str]=[\"nmbr\"],\n",
    "                 lang_list: List[str]=[\"rus\"]) -> None:\n",
    "        \"\"\"Initializes an object\"\"\"\n",
    "        # Это же фасад?\n",
    "        self.__gramcats = gramcats\n",
    "        self.__lang_list = lang_list\n",
    "        self.conllu = ConlluUDParser()\n",
    "        self.spoiled_texts = IlliterateUD()\n",
    "\n",
    "    def get_perplexity_results(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Gets languages and grammatical categories to check\"\"\"\n",
    "\n",
    "        conllu = self.conllu\n",
    "        conllu.get_good_UDtexts(self.__lang_list)\n",
    "        getting_texts = self.spoiled_texts\n",
    "        getting_texts.get_illiterate_UDtexts(self.__gramcats, self.__lang_list)\n",
    "\n",
    "        lang_perplexityscores = {}\n",
    "        for lang in self.__lang_list:\n",
    "            print(lang)\n",
    "            #model_name = self.lang_model_name[lang]\n",
    "            model_name = \"distilbert-base-multilingual-cased\"\n",
    "            model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            block_size = 128\n",
    "            \n",
    "            def tokenize_function(examples):\n",
    "                  return tokenizer(examples[\"text\"])\n",
    "            def group_texts(examples):\n",
    "                    # Concatenate all texts.\n",
    "                    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "                    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "                    # We drop the small remainder, we could add padding\n",
    "                    # if the model supported it instead of this drop,\n",
    "                    # you can customize this part to your needs.\n",
    "                    total_length = (total_length // block_size) * block_size\n",
    "                    # Split by chunks of max_len.\n",
    "                    result = {k: [t[i : i + block_size] for i in range(0,\n",
    "                                                                       total_length,\n",
    "                                                                       block_size)]\n",
    "                              for k, t in concatenated_examples.items()}\n",
    "                    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "                    return result\n",
    "            \n",
    "            training_args = TrainingArguments(model_name,\n",
    "                                  evaluation_strategy = \"epoch\",\n",
    "                                  learning_rate=2e-5,\n",
    "                                  weight_decay=0.01)\n",
    "            filename_perplexity = Counter()\n",
    "            for filename in glob.glob(lang+\"_for_perplexity/*.csv\"):\n",
    "                if filename[33:-4] in self.__gramcats or filename == lang+\"_for_perplexity/\"+lang+\".csv\":\n",
    "                    # берём файл, если он про нужную категорию или если он с правильными предложениями\n",
    "                    print(filename)\n",
    "                    datasets = load_dataset(\"text\",\n",
    "                                            data_files={\"validation\": filename})\n",
    "                    tokenized_datasets = datasets.map(tokenize_function,\n",
    "                                                      batched=True,\n",
    "                                                      num_proc=4,\n",
    "                                                      remove_columns=[\"text\"])\n",
    "    \n",
    "                    lm_datasets = tokenized_datasets.map(group_texts,\n",
    "                                             batched=True,\n",
    "                                             batch_size=1000,\n",
    "                                             num_proc=4)\n",
    "                    trainer = Trainer(model=model,\n",
    "                                      args=training_args,\n",
    "                                      eval_dataset=lm_datasets[\"validation\"])\n",
    "                    eval_results = trainer.evaluate()\n",
    "        \n",
    "                    filename_perplexity[filename] = math.exp(eval_results['eval_loss'])\n",
    "                    print(filename_perplexity.most_common())\n",
    "                    lang_perplexityscores[lang] = filename_perplexity.most_common()\n",
    "\n",
    "            return lang_perplexityscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/tbkazakova/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/fb240273126596a03b35c85793d2e82a5b13ac79/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-multilingual-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/tbkazakova/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/fb240273126596a03b35c85793d2e82a5b13ac79/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
      "\n",
      "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
      "loading configuration file config.json from cache at /Users/tbkazakova/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/fb240273126596a03b35c85793d2e82a5b13ac79/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-multilingual-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/tbkazakova/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/fb240273126596a03b35c85793d2e82a5b13ac79/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/tbkazakova/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/fb240273126596a03b35c85793d2e82a5b13ac79/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/tbkazakova/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/fb240273126596a03b35c85793d2e82a5b13ac79/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/tbkazakova/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/fb240273126596a03b35c85793d2e82a5b13ac79/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-multilingual-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_for_perplexity/rus_spoiledUD_case.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-007280e195132a0b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /Users/tbkazakova/.cache/huggingface/datasets/text/default-007280e195132a0b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████| 1/1 [00:00<00:00, 696.84it/s]\n",
      "Extracting data files: 100%|██████████████████████| 1/1 [00:00<00:00, 80.06it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/tbkazakova/.cache/huggingface/datasets/text/default-007280e195132a0b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 65.43it/s]\n",
      "\n",
      "#1: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 30.04ba/s]\u001b[A\n",
      "#0: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 25.27ba/s]\n",
      "\n",
      "\n",
      "#2:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#2: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 17.87ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#3: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 31.77ba/s]\n",
      "#0:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\n",
      "\n",
      "#2:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "#1:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "\n",
      "#0: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 12.88ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#1: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 15.52ba/s]\n",
      "#2: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 11.15ba/s]\n",
      "#3: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 15.03ba/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rus_for_perplexity/rus_spoiledUD_case.csv', 3.9193330434320828)]\n",
      "rus_for_perplexity/rus_spoiledUD_tens.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8b39d5e7a6f342eb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /Users/tbkazakova/.cache/huggingface/datasets/text/default-8b39d5e7a6f342eb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████| 1/1 [00:00<00:00, 433.07it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 184.94it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/tbkazakova/.cache/huggingface/datasets/text/default-8b39d5e7a6f342eb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 92.69it/s]\n",
      "#0: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 31.53ba/s]\n",
      "\n",
      "#1: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 28.82ba/s]\u001b[A\n",
      "\n",
      "\n",
      "#2: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 33.83ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#3: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 36.19ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#0:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\n",
      "#1:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "#0: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 19.62ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#1: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 12.30ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#2: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 15.46ba/s]\n",
      "#3: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 32.21ba/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 23\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rus_for_perplexity/rus_spoiledUD_case.csv', 3.9193330434320828), ('rus_for_perplexity/rus_spoiledUD_tens.csv', 3.845178846973014)]\n",
      "rus_for_perplexity/rus.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-26419b9e1f98050b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /Users/tbkazakova/.cache/huggingface/datasets/text/default-26419b9e1f98050b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████| 1/1 [00:00<00:00, 839.03it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 137.76it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/tbkazakova/.cache/huggingface/datasets/text/default-26419b9e1f98050b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.85it/s]\n",
      "\n",
      "#1: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 23.84ba/s]\u001b[A\n",
      "#0:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\n",
      "\n",
      "#0: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 26.64ba/s]\u001b[A\u001b[A\n",
      "#2: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 28.84ba/s]\n",
      "\n",
      "\n",
      "\n",
      "#3: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 42.83ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#0:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\n",
      "\n",
      "#2:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "#0: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 12.29ba/s]\u001b[A\n",
      "#2: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 16.77ba/s]\n",
      "\n",
      "\n",
      "\n",
      "#1: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 19.50ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#3: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 38.39ba/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rus_for_perplexity/rus_spoiledUD_case.csv', 3.9193330434320828), ('rus_for_perplexity/rus_spoiledUD_tens.csv', 3.845178846973014), ('rus_for_perplexity/rus.csv', 3.73473068311676)]\n",
      "rus_for_perplexity/rus_spoiledUD_nmbr.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ab24801086a20baf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /Users/tbkazakova/.cache/huggingface/datasets/text/default-ab24801086a20baf/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████| 1/1 [00:00<00:00, 658.24it/s]\n",
      "Extracting data files: 100%|██████████████████████| 1/1 [00:00<00:00, 98.01it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/tbkazakova/.cache/huggingface/datasets/text/default-ab24801086a20baf/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 106.18it/s]\n",
      "#0: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 25.48ba/s]\n",
      "\n",
      "#1: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 39.60ba/s]\u001b[A\n",
      "\n",
      "\n",
      "#2: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 35.95ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#3: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 30.83ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#0:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\n",
      "#1:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "#2:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#0: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 16.53ba/s]\n",
      "#1: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 25.77ba/s]\n",
      "#2: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 29.26ba/s]\n",
      "#3: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 30.14ba/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 23\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rus_for_perplexity/rus_spoiledUD_case.csv', 3.9193330434320828), ('rus_for_perplexity/rus_spoiledUD_nmbr.csv', 3.887017576653393), ('rus_for_perplexity/rus_spoiledUD_tens.csv', 3.845178846973014), ('rus_for_perplexity/rus.csv', 3.73473068311676)]\n",
      "rus_for_perplexity/rus_spoiledUD_pers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1659fa0d21832e86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /Users/tbkazakova/.cache/huggingface/datasets/text/default-1659fa0d21832e86/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████| 1/1 [00:00<00:00, 503.28it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 107.30it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/tbkazakova/.cache/huggingface/datasets/text/default-1659fa0d21832e86/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 98.34it/s]\n",
      "#0: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 36.00ba/s]\n",
      "\n",
      "#1:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "#1: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 18.96ba/s]\u001b[A\u001b[A\n",
      "#2: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 30.77ba/s]\n",
      "\n",
      "\n",
      "\n",
      "#3: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 40.19ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#0:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\n",
      "#0: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 25.09ba/s]\u001b[A\n",
      "\n",
      "\n",
      "#2:   0%|                                                 | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#1: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 18.34ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#2: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 24.85ba/s]\n",
      "#3: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 24.77ba/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rus_for_perplexity/rus_spoiledUD_case.csv', 3.9193330434320828), ('rus_for_perplexity/rus_spoiledUD_nmbr.csv', 3.887017576653393), ('rus_for_perplexity/rus_spoiledUD_tens.csv', 3.845178846973014), ('rus_for_perplexity/rus.csv', 3.73473068311676), ('rus_for_perplexity/rus_spoiledUD_pers.csv', 3.629800816860565)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rus': [('rus_for_perplexity/rus_spoiledUD_case.csv', 3.9193330434320828),\n",
       "  ('rus_for_perplexity/rus_spoiledUD_nmbr.csv', 3.887017576653393),\n",
       "  ('rus_for_perplexity/rus_spoiledUD_tens.csv', 3.845178846973014),\n",
       "  ('rus_for_perplexity/rus.csv', 3.73473068311676),\n",
       "  ('rus_for_perplexity/rus_spoiledUD_pers.csv', 3.629800816860565)]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = PerplexityProbing([\"nmbr\", \"case\", \"pers\", \"tens\"], [\"rus\"])\n",
    "c.get_perplexity_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Pr7krKHkfmF"
   },
   "source": [
    "У нас система:\n",
    "\n",
    "- ConlluUDParser - умеет переводить conllu файлы в разные csv форматы (для перрплексии - get_good_UDtexts(список_языков) - пока в разработке\n",
    "- AIRI_Probing - общий класс для всех экспериментов AIRI - пока в разработке\n",
    "- PerplexityProbing(AIRI_Probing) - наш эксперимент про перплексию\n",
    "    - TextIlliteracy - родительский класс для порчи текстов\n",
    "    - TextIlliteracyLang(TextIlliteracy) - лингвоспецифичные класс для порчи текстов\n",
    "    - IlliterateUD - сохраняет нужные испорченные тексты в нужные папки и файлики\n",
    "\n",
    "\n",
    "Мы:\n",
    "- Выбираем тип пробинга: перплексия (а есть другие в т.ч. то, что у Олега в pipeline)\n",
    "- Даём список список языков, список моделей для каждого языка, список категорий, которые мы хотим посмотреть\n",
    "- Получаем словарь, язык-модель: ранжированный по важности список категорий (сильнее ухудшается перплексия - важнее) с цифрой перплексии\n",
    "\n",
    "Внутри PerplexityProbing:\n",
    "\n",
    "- для каждого языка из UD достаём тексты и сохраняем в папке UD_sentences файл lang_UD_sentences.csv списки предложений (by ConlluUDParser)\n",
    "- для каждой грамматической категории получаем lang_spoiledUD_gramcat.csv со списками предложений, испорченными по категориям, кладём в папки lang_for_perplexity (by IlliterateUD, который использует TextIlliteracyLang(TextIlliteracy))\n",
    "- для каждой пары язык-модель считаем перплексию для каждого из наборов предложений\n",
    "- ранжируем грамматические категории по важности внутри модели-языка, получаем на выходе словарь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiOpGZUxkfmG"
   },
   "source": [
    "# Теоретические вопросы к происходящему\n",
    "\n",
    "1. Надо менять категорию во всех местах? На рандом или неправильную (рандом из неправильных?)\n",
    "- у меня просто рандом (можно обсуждать)\n",
    "\n",
    "2. Как потом сравнивать удивление от изменений в категории, если род прилагательных и время у глагола имеют разную частотность в тексте?\n",
    "Как нормализовывать? 30 слов: 10 глаголов - удивление/10\\*30, 4 прилагательных - удивление/4\\*30 Так?\n",
    "- контролировать кол-во слов с изменениями? - спорно\n",
    "- пока никак не нормализую, подумать, обсудить\n",
    "\n",
    "3. Мы теряем информацию об абзацах (пробелоподобных символах - не пробелах), заменяя их пробелами. Ок?\n",
    "- ок, потому что мы смотрим на предложения, а не длинные тексты\n",
    "\n",
    "4. Если у POS нет такой грам.категории текст просто не меняется. Можно было бы ограничить, чтоб на нерелевантной паре прогамма бы ломалась или хотя бы писала, что пользователь делает что-то странное.\n",
    "- не хочу, чтоб пограмма ломалась, но предупреждение, да, можно выдавать\n",
    "\n",
    "5. Логичнее же смотреть на предложения, а не тексты? Вроде так."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
